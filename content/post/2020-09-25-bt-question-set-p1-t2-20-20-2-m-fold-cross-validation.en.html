---
title: 'BT Question Set P1-T2-20-20-2: m-fold cross-validation'
author: David Harper
date: '2020-09-25'
slug: bt-question-set-p1-t2-20-20-2-m-fold-cross-validation
categories:
  - R
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2020-09-25T21:26:01-07:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---



<div id="m-fold-cross-validation" class="section level3">
<h3>m-fold cross-validation</h3>
<p>Following GARPâ€™s (sort of) CV approach</p>
<pre class="r"><code>library(tidyverse)

obs &lt;- tibble(
    X = c(1.00, 2.00, 3.00, 4.00, 5.00, 6.00, 7.00, 8.00, 9.00), 
    Y = c(2.20, 3.50, 5.20, 2.80, 6.90, 8.80, 5.70, 11.30, 8.60)
)

testrows_M1 &lt;- 1:6; testrows_M2 &lt;- 4:9; testrows_M3 &lt;- c(1:3, 7:9)
train_M1 &lt;- obs[testrows_M1, ]; test_M1 &lt;- obs[-testrows_M1, ]
train_M2 &lt;- obs[testrows_M2, ]; test_M2 &lt;- obs[-testrows_M2, ]
train_M3 &lt;- obs[testrows_M3, ]; test_M3 &lt;- obs[-testrows_M3, ]

model_M1 &lt;- lm(Y ~ X, train_M1)
model_M2 &lt;- lm(Y ~ X, train_M2)
model_M3 &lt;- lm(Y ~ X, train_M3)

predict_M1 &lt;- predict(model_M1, test_M1, type = &quot;response&quot;)
predict_M2 &lt;- predict(model_M2, test_M2, type = &quot;response&quot;)
predict_M3 &lt;- predict(model_M3, test_M3, type = &quot;response&quot;)

error_M1 &lt;- test_M1$Y - predict_M1
error_M2 &lt;- test_M2$Y - predict_M2
error_M3 &lt;- test_M3$Y - predict_M3

CV_RSS_M1 &lt;- sum(error_M1^2) # GARP&#39;s selection metric
CV_RSS_M2 &lt;- sum(error_M2^2)
CV_RSS_M3 &lt;- sum(error_M3^2)

df &lt;- nrow(obs) - 2 # df = n - k, where k is no. of coefficients
# sample too small, not using

RMSE_M1 &lt;- sqrt(mean(error_M1^2))
RMSE_M2 &lt;- sqrt(mean(error_M2^2))
RMSE_M3 &lt;- sqrt(mean(error_M3^2))

RMSE_M1</code></pre>
<pre><code>## [1] 2.545756</code></pre>
<pre class="r"><code>RMSE_M2</code></pre>
<pre><code>## [1] 1.350539</code></pre>
<pre class="r"><code>RMSE_M3</code></pre>
<pre><code>## [1] 1.82291</code></pre>
<pre class="r"><code>library(caret)

model_cv &lt;- train(
  Y ~ X, 
  obs,
  method = &quot;lm&quot;,
  trControl = trainControl(
    method = &quot;cv&quot;, 
    number = 3,
    verboseIter = TRUE
  )
)</code></pre>
<pre><code>## + Fold1: intercept=TRUE 
## - Fold1: intercept=TRUE 
## + Fold2: intercept=TRUE 
## - Fold2: intercept=TRUE 
## + Fold3: intercept=TRUE 
## - Fold3: intercept=TRUE 
## Aggregating results
## Fitting final model on full training set</code></pre>
<pre class="r"><code># Note that the final model is a regression on the full dataset
# Strictly CV is to estimate the out-of-sample error; the folder models are disposed
# We are not REALLY comparing three different models; there is only one model here
# CV is to check the model not build the model
# And intuitively: we want to use all the data to fit the model.

model_cv$finalModel</code></pre>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Coefficients:
## (Intercept)            X  
##      1.4444       0.9333</code></pre>
<pre class="r"><code>model_full &lt;- lm(Y~ X, obs)
model_full</code></pre>
<pre><code>## 
## Call:
## lm(formula = Y ~ X, data = obs)
## 
## Coefficients:
## (Intercept)            X  
##      1.4444       0.9333</code></pre>
</div>
